# Can AI Be Truly Moral? A Reflection on Kant’s Argument from Moral Feelings

*Based on Dieter Schönecker’s chapter in* Kant and Artificial Intelligence *(2022)*

---

### Introduction: Can a Machine Ever Really Feel?

I’ve often asked myself: Can machines ever really understand what is right or wrong? Could they ever feel something like guilt, respect, or responsibility? This question becomes especially interesting when we look at it through the lens of Kantian philosophy, particularly the argument that moral feelings are necessary for true practical reason. In Dieter Schönecker’s chapter, “Kant’s Argument from Moral Feelings: Why Practical Reason Cannot Be Artificial,” he lays out why Kant would likely say: No, machines can never truly be moral agents. And I find his reasoning both challenging and helpful as I try to make sense of what makes human morality different.

---

### 1. Moral Feelings Are Not Just Extra

According to Kant, feelings like respect for the moral law are not just nice additions. They are necessary for morality. Schönecker points out that for Kant, this feeling of respect doesn’t come from the senses. It comes from reason itself. This is different from emotions like happiness or sadness, which we can explain by brain processes. Moral feelings, in contrast, are tied to how we understand and respond to duty. So even if a computer could mimic emotions or show something that looks like caring, it wouldn’t be feeling respect in the Kantian sense.

---

### 2. The Power of Judgment: More Than Following Rules

I used to think that morality was just about following rules. But Kant makes an important point: applying rules in the right way, in the right situation, requires judgment. And judgment is not something we can fully explain by more rules. That leads to what he calls the "power of judgment" — the ability to connect what is general (a rule) with what is specific (a case).

Now, Schönecker explains that there are two kinds of judgment in Kant: the determining power (applying an existing rule) and the reflective power (finding the right rule when none exists yet). Computers may do the first, but not the second. They need clear instructions. They don’t know when to stop applying a rule or when it doesn’t fit. This means their actions are not really free in the moral sense.

---

### 3. I Think vs. I Feel: What Machines Are Missing

This part of the chapter made me stop and reflect. Kant says that any act of thinking must come with the awareness that "I think." This is not just being able to compute; it’s about being aware that I am the one doing the thinking. Schönecker goes deep into this idea, showing that for Kant, this unity of self-awareness (what he calls apperception) is what makes thinking truly human.

Now, computers might combine data or symbols, but they don’t have an "I" that thinks. They don’t say to themselves, “I am the one combining this.” That, to me, is a big difference. And when it comes to feelings, this gap becomes even bigger. A feeling must be felt. And for that, there has to be someone who feels it. Without a conscious subject, there is no real feeling — only behavior that looks like feeling.

---

### 4. Can Computers Ever Feel?

At this point, I asked myself the same question Schönecker raises: Could a computer, someday, really feel? He walks us through the two major problems with this idea.

First, if moral feelings are essential to morality, then what about holy beings — like angels — who are always good and have no need for moral struggle? Does that make them like moral machines? Schönecker argues no. Holy beings have a good will. Computers don’t have any will at all. They don’t choose.

Second, maybe computers could feel, if feelings are just complex results of matter. But Schönecker doubts it. A single brain cell is more complex than any computer. And even if it's possible in theory, it seems very unlikely. Machines, he says, are just not built the way minds are.

---

### 5. Flying Planes and Swimming Submarines

This final image stayed with me. The computer scientist Edsger Dijkstra once said that asking if machines can think is like asking if submarines can swim. Submarines do move through water, but not the way fish do. Planes fly, but not like birds. And maybe computers think — but not like humans. Schönecker warns that this is misleading. A machine can process data, but to say it "thinks" or "feels" or "wants" is to go too far. Without an "I," a computer's actions are not truly moral or conscious.

To say a computer feels, he writes, is like saying that a planet flies just because it moves through space.

---

### Conclusion: Why AI Lacks Practical Reason

Reading this chapter has helped me see more clearly why Kant’s ethics can't be applied to machines as they are. If being moral means having the capacity to feel respect, judge wisely, and act freely, then machines fall short. They have no inner voice, no “I think,” and no awareness of duty. Even the most advanced AI is not self-aware in the way humans are. 

That doesn’t mean we can’t or shouldn’t use machines to help with moral decisions. But it does mean we should be careful not to mistake intelligence for responsibility. 

I think this is what Schönecker’s chapter helps us grasp: that morality is not just about behavior. It’s about what happens inside the one who acts.

---

**Reference:**
Dieter Schönecker. “Kant’s Argument from Moral Feelings: Why Practical Reason Cannot Be Artificial.” In *Kant and Artificial Intelligence*, edited by Hyeongjoo Kim and Dieter Schönecker, De Gruyter, 2022. https://doi.org/10.1515/9783110706611
